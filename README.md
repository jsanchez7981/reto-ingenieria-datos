# Reto de IngenierÃ­a de Datos

## ğŸš€ DescripciÃ³n

Este proyecto implementa un **pipeline de datos por micro batches** que simula un entorno de carga progresiva de archivos CSV en una base de datos relacional, con cÃ¡lculo incremental de estadÃ­sticas y control de memoria.

El objetivo es demostrar habilidades en:

- Procesamiento de datos en micro batches.
- Ingesta controlada a base de datos.
- CÃ¡lculo incremental de estadÃ­sticas sin usar agregaciones SQL en tiempo real.
- Consulta directa a base de datos para validaciÃ³n.
- Persistencia de estadÃ­sticas en archivos y/o base de datos.

## ğŸ—ï¸ Estructura del proyecto

```
Pragma/
â”œâ”€â”€ data/                      # Archivos CSV fuente (2012-1.csv a 2012-5.csv + validation.csv)
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ main.py                # Pipeline principal
â”‚   â”œâ”€â”€ db_utils.py            # ConexiÃ³n a PostgreSQL
â”‚   â””â”€â”€ stats_tracker.py       # Clase para estadÃ­sticas incrementales
â”œâ”€â”€ estadisticas.json          # Archivo de salida con estadÃ­sticas acumuladas
â”œâ”€â”€ requirements.txt           # Dependencias Python
â””â”€â”€ README.md                  # Este archivo
```

## âš™ï¸ Requisitos

- Python 3.12
- PostgreSQL (local o remoto)
- Conda o entorno virtual

InstalaciÃ³n de dependencias:

```bash
pip install -r requirements.txt
```

## ğŸ§ª PreparaciÃ³n de la base de datos

1. Crear base de datos (ejemplo: `pragma`):

```sql
CREATE DATABASE pragma;
```

2. Crear tabla principal de transacciones:

```sql
CREATE TABLE transactions (
    id SERIAL PRIMARY KEY,
    timestamp DATE,
    price NUMERIC,
    user_id INTEGER
);
```

3. (Opcional) Crear tabla para guardar estadÃ­sticas:

```sql
CREATE TABLE estadisticas (
    id SERIAL PRIMARY KEY,
    etapa TEXT,
    total_registros INT,
    promedio NUMERIC,
    minimo NUMERIC,
    maximo NUMERIC,
    fecha TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

## ğŸ§  LÃ³gica del pipeline

El archivo `main.py` ejecuta los siguientes pasos:

1. Recorre todos los archivos `*.csv` excepto `validation.csv`.
2. Carga cada archivo en **micro batches de 20 filas** usando `pandas.read_csv(..., chunksize=20)`.
3. Cada chunk:
   - Limpia y convierte `timestamp` y `price`.
   - Inserta el batch en PostgreSQL usando `SQLAlchemy`.
   - Actualiza estadÃ­sticas incrementales (count, promedio, min, max).
   - Muestra progreso con `tqdm`.

4. DespuÃ©s de los 5 archivos:
   - Se imprime y guarda un snapshot final de las estadÃ­sticas acumuladas.
   - Se ejecuta una consulta SQL para validar conteo y agregados desde la base de datos.
5. Se repite el mismo proceso para `validation.csv`.

## ğŸ“Š EstadÃ­sticas generadas

EstadÃ­sticas calculadas y almacenadas:

| Campo     | DescripciÃ³n                          |
|-----------|--------------------------------------|
| `count`   | Total de registros procesados        |
| `mean`    | Promedio incremental del `price`     |
| `min`     | Valor mÃ­nimo de `price`              |
| `max`     | Valor mÃ¡ximo de `price`              |

Se almacenan en:
- Consola (durante ejecuciÃ³n)
- `estadisticas.json`
- (Opcional) Tabla `estadisticas` en la base de datos

## ğŸ“¡ Consulta directa en PostgreSQL

Para validar resultados:

```sql
SELECT 
    COUNT(price) AS total,
    AVG(price) AS promedio,
    MIN(price) AS minimo,
    MAX(price) AS maximo
FROM transactions;
```

## ğŸ“¦ Â¿CÃ³mo ejecutar?

Con entorno activado y archivos CSV en la carpeta `data/`:

```bash
python pipeline/main.py
```

## âœ… Consideraciones tÃ©cnicas

- La columna `price` se convierte a nÃºmero.
- Los valores nulos se descartan (`dropna`) antes de insertarlos o calcular estadÃ­sticas.
- Se evita cargar todos los datos en memoria usando `chunksize`.
- El pipeline es tolerante a errores y puede ampliarse fÃ¡cilmente con logging o interfaz.

## ğŸ”š CrÃ©ditos

Desarrollado por Jimmy Ricardo SÃ¡nchez Romero como parte de un reto de ingenierÃ­a de datos para PRAGMA.
